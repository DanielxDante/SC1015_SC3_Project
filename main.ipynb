{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaba6c08",
   "metadata": {},
   "source": [
    "# Dataset and Question\n",
    "---\n",
    "Dataset: https://www.kaggle.com/benroshan/online-food-delivery-preferencesbangalore-region\n",
    "\n",
    "With so many restaurants in Singapore, having the edge to attract more customers is lucrative.  \n",
    "One way a restaurant can naturally attract more customers is through **reviews** from the customers.  \n",
    "The plethora of online food delivery services act as a review bank for the many restaurants, hence, a good place to start exploring.  \n",
    "We want to find out, if we were ever to setup our own restaurant with our own online delivery service, what would make customers come back and order again.  \n",
    "\n",
    ">**What are the optimal factors for a restaurant to attract consumers via food delivery service?**\n",
    "\n",
    "Our dataset is set in a metropolitan city in India, Bangalore.  \n",
    "Due to the recent (2021) rise in demand of online delivery there, this dataset was gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0c0db",
   "metadata": {},
   "source": [
    "# Essential Libraries\n",
    "---\n",
    "    > NumPy : Library for Numeric Computations in Python\n",
    "    > Pandas : Library for Data Acquisition and Preparation\n",
    "    > Matplotlib : Low-level library for Data Visualization\n",
    "    > Seaborn : Higher-level library for Data Visualization\n",
    "    > Scikit Learn : Regressions and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8de806",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import re\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5acfeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodDelivery = pd.read_csv(\"onlinedeliverydata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18fae9",
   "metadata": {},
   "source": [
    "# EDA\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b76223",
   "metadata": {},
   "source": [
    "## Dataset Analysis\n",
    "---\n",
    "\n",
    "There are a lot of Categorical variables compared to Numerical variables, as seen from the many objects in the dataset info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada241e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodDelivery.head(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodDelivery.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodDelivery.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfdeb6",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "---\n",
    "\n",
    "#### For our dataset, since there are 55 variables, we have broken down the dataset into a few smaller categories:\n",
    "- Consumer Demographics:\n",
    "    - Basic Information:\n",
    "        - Age\n",
    "        - Gender\n",
    "    - Family:\n",
    "        - Marital Status\n",
    "        - Occupation\n",
    "        - Monthly Income\n",
    "        - Educational Qualifications\n",
    "        - Family size\n",
    "    - Residence:\n",
    "        - Latitude\n",
    "        - Longitude\n",
    "        - Pin code\n",
    "    - Delivery Preferences:\n",
    "        - Medium of order (Preference 1)\n",
    "        - Medium of order (Preference 2)\n",
    "        - Meal-of-the-day of order (Preference 1)\n",
    "        - Meal-of-the-day of order (Preference 2)\n",
    "        - General Type of Food (Preference 1)\n",
    "        - General Type of Food (Preference 2)\n",
    "        - Order Time (Time of day to order)\n",
    "        - Maximum Wait Time (Before cancelling the order)\n",
    "\n",
    "\n",
    "- Location:\n",
    "    - Residence in busy location\n",
    "    - Google Maps Accuracy\n",
    "    - Good Road Condition  \n",
    "    \n",
    "\n",
    "- Customer Experience:\n",
    "    - Time Factors:\n",
    "        - Saves Time\n",
    "        - Good Tracking System\n",
    "        - Late Delivery\n",
    "        - Long delivery time\n",
    "        - Delay of delivery person getting assigned\n",
    "        - Delay of delivery person picking up food\n",
    "        - Low Quantity Low Time (Quantity of food affects delivery time)\n",
    "    - Food Factors:\n",
    "        - More restaurant choices available\n",
    "        - Good Food Quality\n",
    "        - Health concern\n",
    "        - Poor Hygiene\n",
    "        - Unavailability\n",
    "        - Unaffordability\n",
    "    - Others:\n",
    "        - Ease and Convenience\n",
    "        - Ease of Payment Option\n",
    "        - More Offers and Discounts\n",
    "        - Self-cooking (Customer cooks)\n",
    "        - Bad Past Experiences\n",
    "        - Delivery person ability\n",
    "        - Wrong order delivered\n",
    "        - Missing item\n",
    "        - Order placed by mistake\n",
    "        - Influence of Time (Order affects delivery time)\n",
    "        - Influence of rating (Current restuarant rating affects order)\n",
    "        - **Output** (Is the customer satisfied with the food order?)\n",
    "        - Reviews\n",
    "\n",
    "\n",
    "- Customer's Demands Importance:\n",
    "    - Less Delivery Time\n",
    "    - High Quality of Package\n",
    "    - Number of calls\n",
    "    - Politeness\n",
    "    - Freshness\n",
    "    - Temperature\n",
    "    - Good Taste\n",
    "    - Good Quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a40ce66",
   "metadata": {},
   "source": [
    "### Consumer Demographics - Basic Information\n",
    "---\n",
    "From the following plots,\n",
    "- Concentration of people between the ages between 22-25\n",
    "- About 50 more males than females  \n",
    "\n",
    "We can conclude that,\n",
    "- Data came from mostly young people\n",
    "- Not much disparity in gender representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772a748",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 1, figsize = (10, 10))\n",
    "f = sb.countplot(x = \"Age\", data = foodDelivery, ax = axes[0])\n",
    "f = sb.countplot(x = \"Gender\", data = foodDelivery, ax = axes[1], order = foodDelivery[\"Gender\"].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813d805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42f1149a",
   "metadata": {},
   "source": [
    "### Consumer Demographics - Family \n",
    "--- \n",
    "From the following plots,\n",
    "- Most are single\n",
    "- Most are either students or employed\n",
    "- Most have no income\n",
    "- Most are graduates or post-graduates\n",
    "- Most have family size of 3 or 2  \n",
    "\n",
    "We can conclude that,\n",
    "- Data came from university students\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe3cbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(5, 1, figsize = (10, 30))\n",
    "f = sb.countplot(x = \"Marital Status\", data = foodDelivery, ax = axes[0])\n",
    "f = sb.countplot(x = \"Occupation\", data = foodDelivery, ax = axes[1])\n",
    "f = sb.countplot(x = \"Monthly Income\", data = foodDelivery, ax = axes[2], order = foodDelivery[\"Monthly Income\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Educational Qualifications\", data = foodDelivery, ax = axes[3], order = foodDelivery[\"Educational Qualifications\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Family size\", data = foodDelivery, ax = axes[4], order = foodDelivery[\"Family size\"].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415df0e0",
   "metadata": {},
   "source": [
    "### Consumer Demographics - Residence\n",
    "---\n",
    "The Geolocation of the different clients is recorded on the survey, and we can use gmpas API to plot their locations in a heatmap on a map.\n",
    "\n",
    "We can see how clients are spread thorughout Bangalore, but are more concentrated closer towards the centre of the city.\n",
    "\n",
    "Hence, it maybe beneficial for future restaurant owners to position their restaurants near darker spots on the heatmap to minimise on delivery times.  \n",
    "\n",
    "Since Latitude, Longitude, and Pin code only show use where the customers come from, it cannot qualitatively help us in solving the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc67d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      latitude  longitude\n",
       "0     12.9766    77.5993\n",
       "1     12.9770    77.5773\n",
       "2     12.9551    77.6593\n",
       "3     12.9473    77.5616\n",
       "4     12.9850    77.5533\n",
       "..        ...        ...\n",
       "383   12.9766    77.5993\n",
       "384   12.9854    77.7081\n",
       "385   12.9850    77.5533\n",
       "386   12.9770    77.5773\n",
       "387   12.8988    77.5764\n",
       "\n",
       "[388 rows x 2 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gmaps\n",
    "gmaps.configure(api_key='AIzaSyA9m5OlBgrWywCl9u--IuArU6N2BaUmgNo') # Fill in with your API key\n",
    "\n",
    "loc = pd.DataFrame(foodDelivery[['latitude','longitude']])\n",
    "\n",
    "loc.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e386f44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0cd7e83aa24e30b8c9b80b96804755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(layout=FigureLayout(height='420px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = gmaps.figure()\n",
    "heatmap = gmaps.heatmap_layer(loc)\n",
    "\n",
    "heatmap.max_intensity = 0.03\n",
    "heatmap.point_radius = 0.014\n",
    "heatmap.dissipating = False\n",
    "\n",
    "\n",
    "##fig = gmaps.figure(map_type='SATELLITE')\n",
    "fig.add_layer(heatmap)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "163c9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800a484c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'collections' has no attribute 'Iterable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m markers \u001b[38;5;241m=\u001b[39m \u001b[43mgmaps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarker_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m fig\u001b[38;5;241m.\u001b[39madd_layer(markers)\n\u001b[0;32m      3\u001b[0m fig\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gmaps\\marker.py:553\u001b[0m, in \u001b[0;36mmarker_layer\u001b[1;34m(locations, hover_text, label, info_box_content, display_info_box)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;129m@doc_subst\u001b[39m(_doc_snippets)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmarker_layer\u001b[39m(\n\u001b[0;32m    487\u001b[0m         locations, hover_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    488\u001b[0m         info_box_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, display_info_box\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124;03m    Marker layer\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m        A :class:`gmaps.Markers` instance.\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     marker_options \u001b[38;5;241m=\u001b[39m \u001b[43m_marker_layer_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhover_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_box_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_info_box\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m     markers \u001b[38;5;241m=\u001b[39m [Marker(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moption) \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m marker_options]\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Markers(markers\u001b[38;5;241m=\u001b[39mmarkers)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gmaps\\marker.py:311\u001b[0m, in \u001b[0;36m_marker_layer_options\u001b[1;34m(locations, hover_text, label, info_box_content, display_info_box)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_atomic(label):\n\u001b[0;32m    310\u001b[0m     label \u001b[38;5;241m=\u001b[39m [label] \u001b[38;5;241m*\u001b[39m number_markers\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_atomic\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_box_content\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    312\u001b[0m     info_box_content \u001b[38;5;241m=\u001b[39m [info_box_content] \u001b[38;5;241m*\u001b[39m number_markers\n\u001b[0;32m    314\u001b[0m marker_options \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m: locations_to_list(locations),\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhover_text\u001b[39m\u001b[38;5;124m'\u001b[39m: hover_text,\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label\n\u001b[0;32m    318\u001b[0m }\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gmaps\\options.py:40\u001b[0m, in \u001b[0;36mis_atomic\u001b[1;34m(elem)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_atomic\u001b[39m(elem):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    True if an element is a single atom and false if it's a collection\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(elem, string_types) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[43mcollections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIterable\u001b[49m)\n\u001b[0;32m     41\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'collections' has no attribute 'Iterable'"
     ]
    }
   ],
   "source": [
    "markers = gmaps.marker_layer(loc)\n",
    "fig.add_layer(markers)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beeff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodDelivery = foodDelivery.drop(columns = [\"latitude\", \"longitude\", \"Pin code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab0ff41",
   "metadata": {},
   "source": [
    "### Consumer Demographics - Delivery Preferences\n",
    "---\n",
    "Since Medium of order (be on all mediums), Meal-of-the-day of order (open all day), and Order Time (open all day) are variables we do not need to predict, we will be dropping these variables.\n",
    "From the following plots,\n",
    "- Most prefer Non-veg foods\n",
    "- Maximum wait time for most is either 40 or 30 minutes\n",
    "\n",
    "We can conclude that,\n",
    "- University students mostly eat Non-veg foods\n",
    "- Delivery time should not exceed 40 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodDelivery = foodDelivery.drop(columns = [\"Medium (P1)\", \"Medium (P2)\", \"Meal(P1)\", \"Meal(P2)\", \"Order Time\"])\n",
    "\n",
    "f, axes = plt.subplots(3, 1, figsize = (15, 30))\n",
    "f = sb.countplot(x = \"Perference(P1)\", data = foodDelivery, ax = axes[0], order = foodDelivery[\"Perference(P1)\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Perference(P2)\", data = foodDelivery, ax = axes[1], order = foodDelivery[\"Perference(P2)\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Maximum wait time\", data = foodDelivery, ax = axes[2], order = foodDelivery[\"Maximum wait time\"].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9742e",
   "metadata": {},
   "source": [
    "### Location\n",
    "---\n",
    "Since we cannot control these variables in real-time, we drop these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodDelivery = foodDelivery.drop(columns = [\"Residence in busy location\", \"Google Maps Accuracy\", \"Good Road Condition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278d18f",
   "metadata": {},
   "source": [
    "### Customer Experience: Time Factors\n",
    "---\n",
    "We will be performing Text Analysis on the Reviews.  \n",
    "From the following plots,\n",
    "- Most agree that online delivery saves time\n",
    "- Most agree that there is a good delivery tracking system\n",
    "- Most agree that their deliveries are late\n",
    "- Most agree that their deliveries take a long time\n",
    "- Most agree that there is a delay in delivery person getting assigned\n",
    "- Most agree that there is a delay in delivery person picking up food\n",
    "- Most agree that the lower the quantity of food they buy, the quicker their food is delivered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a89385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(7, 1, figsize = (15, 50))\n",
    "f = sb.countplot(x = \"Time saving\", data = foodDelivery, ax = axes[0], order = foodDelivery[\"Time saving\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Good Tracking system\", data = foodDelivery, ax = axes[1], order = foodDelivery[\"Good Tracking system\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Late Delivery\", data = foodDelivery, ax = axes[2], order = foodDelivery[\"Late Delivery\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Long delivery time\", data = foodDelivery, ax = axes[3], order = foodDelivery[\"Long delivery time\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Delay of delivery person getting assigned\", data = foodDelivery, ax = axes[4], order = foodDelivery[\"Delay of delivery person getting assigned\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Delay of delivery person picking up food\", data = foodDelivery, ax = axes[5], order = foodDelivery[\"Delay of delivery person picking up food\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Low quantity low time\", data = foodDelivery, ax = axes[6], order = foodDelivery[\"Low quantity low time\"].value_counts().index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509ed4a",
   "metadata": {},
   "source": [
    "### Customer Experience: Food Factors\n",
    "---\n",
    "We will be performing Text Analysis on the Reviews.  \n",
    "From the following plots,\n",
    "- Most agree that there are many restaurant choices\n",
    "- Most agree that food quality is good\n",
    "- Equal number of people agree and disagree they are concerned with health when ordering food online\n",
    "- Equal number of people agree and disagree the restaurant has poor hygiene\n",
    "- Most people disagree there is unavailability when ordering food\n",
    "- Most disagree that online delivered food is unaffordable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d915888",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(6, 1, figsize = (15, 50))\n",
    "f = sb.countplot(x = \"More restaurant choices\", data = foodDelivery, ax = axes[0], order = foodDelivery[\"More restaurant choices\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Good Food quality\", data = foodDelivery, ax = axes[1], order = foodDelivery[\"Good Food quality\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Health Concern\", data = foodDelivery, ax = axes[2], order = foodDelivery[\"Health Concern\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Poor Hygiene\", data = foodDelivery, ax = axes[3], order = foodDelivery[\"Poor Hygiene\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Unavailability\", data = foodDelivery, ax = axes[4], order = foodDelivery[\"Unavailability\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Unaffordable\", data = foodDelivery, ax = axes[5], order = foodDelivery[\"Unaffordable\"].value_counts().index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76186ed",
   "metadata": {},
   "source": [
    "### Customer Experience: Others\n",
    "---\n",
    "We will be performing Text Analysis on the Reviews.  \n",
    "From the following plots,\n",
    "- Most agree that food deliveries provide ease and convenient\n",
    "- Most agree payment options are easy \n",
    "- Most agree there are more offers and discounts\n",
    "- Almost an equal of people are cooking at home and ordering food online\n",
    "- Most disagree they had bad past experiences with ordering food online\n",
    "- Most agree the delivery person was good at delivering food\n",
    "- Most disagree they had the wrong order delivered to them\n",
    "- Most disagree they had a missing item in their orders\n",
    "- Most disagree they place their orders by mistake\n",
    "- Most said delivery time influences their order \n",
    "- Most said rating of restaurant influences their order\n",
    "- Most people are satisfied with their food order  \n",
    "\n",
    "We conclude that,\n",
    "- ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb1413",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(12, 1, figsize = (15, 70))\n",
    "f = sb.countplot(x = \"Ease and convenient\", data = foodDelivery, ax = axes[0], order = foodDelivery[\"Ease and convenient\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Easy Payment option\", data = foodDelivery, ax = axes[1], order = foodDelivery[\"Easy Payment option\"].value_counts().index)\n",
    "f = sb.countplot(x = \"More Offers and Discount\", data = foodDelivery, ax = axes[2], order = foodDelivery[\"More Offers and Discount\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Self Cooking\", data = foodDelivery, ax = axes[3], order = foodDelivery[\"Self Cooking\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Bad past experience\", data = foodDelivery, ax = axes[4], order = foodDelivery[\"Bad past experience\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Delivery person ability\", data = foodDelivery, ax = axes[5], order = foodDelivery[\"Delivery person ability\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Wrong order delivered\", data = foodDelivery, ax = axes[6], order = foodDelivery[\"Wrong order delivered\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Missing item\", data = foodDelivery, ax = axes[7], order = foodDelivery[\"Missing item\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Order placed by mistake\", data = foodDelivery, ax = axes[8], order = foodDelivery[\"Order placed by mistake\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Influence of time\", data = foodDelivery, ax = axes[9], order = foodDelivery[\"Influence of time\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Influence of rating\", data = foodDelivery, ax = axes[10], order = foodDelivery[\"Influence of rating\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Output\", data = foodDelivery, ax = axes[11], order = foodDelivery[\"Output\"].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d0da7",
   "metadata": {},
   "source": [
    "### Customer's Demands Importance\n",
    "---\n",
    "We will be performing Text Analysis on the Reviews.  \n",
    "From the following plots,\n",
    "- Most think less delivery time is important for satisfaction\n",
    "- Most think higher quality of delivery is important for satisfaction\n",
    "- Most think number of calls is important for satisfaction\n",
    "- Most think politeness of delivery guy is important for satisfaction\n",
    "- Most think freshness of food is very important for satisfaction\n",
    "- Most think temperature of food is important for satisfaction\n",
    "- Most think good taste of food is very important for satisfaction\n",
    "- Most think good quantity of food is very important for satisfaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b4237",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(8, 1, figsize = (15, 50))\n",
    "f = sb.countplot(x = \"Less Delivery time\", data = foodDelivery, ax = axes[0], order = foodDelivery[\"Less Delivery time\"].value_counts().index)\n",
    "f = sb.countplot(x = \"High Quality of package\", data = foodDelivery, ax = axes[1], order = foodDelivery[\"High Quality of package\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Number of calls\", data = foodDelivery, ax = axes[2], order = foodDelivery[\"Number of calls\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Politeness\", data = foodDelivery, ax = axes[3], order = foodDelivery[\"Politeness\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Freshness \", data = foodDelivery, ax = axes[4], order = foodDelivery[\"Freshness \"].value_counts().index)\n",
    "f = sb.countplot(x = \"Temperature\", data = foodDelivery, ax = axes[5], order = foodDelivery[\"Temperature\"].value_counts().index)\n",
    "f = sb.countplot(x = \"Good Taste \", data = foodDelivery, ax = axes[6], order = foodDelivery[\"Good Taste \"].value_counts().index)\n",
    "f = sb.countplot(x = \"Good Quantity\", data = foodDelivery, ax = axes[7], order = foodDelivery[\"Good Quantity\"].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d322ebb",
   "metadata": {},
   "source": [
    "# Multivariate Analysis?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94285d3",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2e769",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "---\n",
    "\n",
    "## Missing Values\n",
    "---\n",
    "Here, we check for NaN values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodDelivery.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e634a6",
   "metadata": {},
   "source": [
    "## Encoding \n",
    "---\n",
    "To start, since the categories are Ordinal (Ordered categories of uneven intervals) we shall encode the different catergorical levels into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ea8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could have used an DataCleaner, but we saw that it encoded the cataegorical variables randomly.\n",
    "cleanup_nums = {\"Gender\": {\"Male\": 0, \"Female\": 1},\n",
    "                \"Marital Status\": {\"Single\": 0, \"Married\": 1, \"Prefer not to say\": 2},\n",
    "                \"Occupation\": {\"Student\": 0, \"Employee\": 1, \"House wife\": 2, \"Self Employeed\": 3},\n",
    "                \"Monthly Income\": {\"No Income\": 0, \"Below Rs.10000\":1, \"10001 to 25000\": 2, \"25001 to 50000\": 3, \"More than 50000\": 4}, \n",
    "                \"Educational Qualifications\": {\"Uneducated\": 0, \"School\": 1, \"Graduate\": 2, \"Post Graduate\": 3, \"Ph.D\": 4},\n",
    "                \"Perference(P1)\": {\"Non Veg foods (Lunch / Dinner)\": 0, \"Veg foods (Breakfast / Lunch / Dinner)\": 1, \"Sweets\": 2, \"Bakery items (snacks)\": 3},\n",
    "                \"Perference(P2)\": {\"Non Veg foods (Lunch / Dinner)\": 0, \" Veg foods (Breakfast / Lunch / Dinner)\": 1, \" Sweets\": 2, \" Bakery items (snacks)\": 3, \" Ice cream / Cool drinks\": 4},\n",
    "                \"Ease and convenient\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Time saving\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"More restaurant choices\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Easy Payment option\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"More Offers and Discount\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Good Food quality\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Good Tracking system\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Self Cooking\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Health Concern\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Late Delivery\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Poor Hygiene\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Bad past experience\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Unavailability\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Unaffordable\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Long delivery time\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Delay of delivery person getting assigned\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Delay of delivery person picking up food\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Wrong order delivered\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Missing item\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Order placed by mistake\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly agree\": 4},\n",
    "                \"Influence of time\": {\"No\": 0, \"Maybe\": 1, \"Yes\": 2},\n",
    "#                \"Order Time\": {\"Anytime (Mon-Sun)\": 0, \"Weekdays (Mon-Fri)\": 1, \"Weekend (Sat & Sun)\": 2},\n",
    "                \"Maximum wait time\": {\"15 minutes\": 0, \"30 minutes\": 1, \"45 minutes\": 2, \"60 minutes\": 3, \"More than 60 minutes\": 4},\n",
    "#                \"Residence in busy location\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly Agree\": 4},\n",
    "#                \"Google Maps Accuracy\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly Agree\": 4},\n",
    "#                \"Good Road Condition\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly Agree\": 4},\n",
    "                \"Low quantity low time\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly Agree\": 4},\n",
    "                \"Delivery person ability\": {\"Strongly disagree\": 0, \"Disagree\": 1, \"Neutral\": 2, \"Agree\": 3, \"Strongly Agree\": 4},\n",
    "                \"Influence of rating\": {\"No\": 0, \"Maybe\": 1, \"Yes\": 2},\n",
    "                \"Less Delivery time\": {\"Unimportant\": 0, \"Slightly Important\": 1, \"Important\": 2, \"Moderately Important\": 3, \"Very Important\": 4},\n",
    "                \"High Quality of package\": {\"Unimportant\": 0, \"Slightly Important\": 1, \"Important\": 2, \"Moderately Important\": 3, \"Very Important\": 4},\n",
    "                \"Number of calls\": {\"Unimportant\": 0, \"Slightly Important\": 1, \"Important\": 2, \"Moderately Important\": 3, \"Very Important\": 4},\n",
    "                \"Politeness\": {\"Unimportant\": 0, \"Slightly Important\": 1, \"Important\": 2, \"Moderately Important\": 3, \"Very Important\": 4},\n",
    "                \"Freshness \": {\"Unimportant\": 0, \"Slightly Important\": 1, \"Important\": 2, \"Moderately Important\": 3, \"Very Important\": 4},\n",
    "                \"Temperature\": {\"Unimportant\": 0, \"Slightly Important\": 1, \"Important\": 2, \"Moderately Important\": 3, \"Very Important\": 4},\n",
    "                \"Good Taste \": {\"Unimportant\": 0, \"Slightly Important\": 1, \"Important\": 2, \"Moderately Important\": 3, \"Very Important\": 4},\n",
    "                \"Good Quantity\": {\"Unimportant\": 0, \"Slightly Important\": 1, \"Important\": 2, \"Moderately Important\": 3, \"Very Important\": 4},\n",
    "                \"Output\": {\"No\": 0, \"Yes\": 1}\n",
    "               }\n",
    "foodDelivery = foodDelivery.replace(cleanup_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45244dd",
   "metadata": {},
   "source": [
    "# Correlation matrix\n",
    "---\n",
    "Pearson's correlation is used to evaluate the linear relationship between two continuous variables. As our data points are ranked ordinal values, we cannot use Pearson's correlation. \n",
    "\n",
    "Instead, Spearman's coefficient is used to determine the relationship between variables since it is based on the ranked values for each variable instead of continuous raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = foodDelivery.corr(method = \"spearman\")\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "sb.set(font_scale = 2.0)\n",
    "sb.heatmap(corr, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9ab41",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "---\n",
    "Since models cannot use text to train and test, we need to convert text into either numbers or arrays.  \n",
    "We understand there are many methods to do this, we will be using two simple methods.\n",
    "\n",
    "## Method 1: Using NLTK and SkLearn's TFIDFVectorizer\n",
    "---\n",
    "First, we need to clean our text data.\n",
    "> - Remove non-alpha characters  \n",
    "> - Convert all alphabets to lowercase (can do uppercase too)  \n",
    "> - Remove stopwords (common english words that contribute nothing into predicting customer satisfaction)  \n",
    "> - Tokenize (convert sentences into array of words)  \n",
    "> - Lemmatize (convert related word forms into its base form e.g., car & cars & car's & cars' --> car)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lower = [word.lower() for word in tokens]\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    clean_text = lemm_text\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Positive and Negative Words found in our data\n",
    "def generate_wordcloud(words, satisfaction):\n",
    "    plt.figure(figsize=(16,13))\n",
    "    wc = WordCloud(background_color=\"white\", max_words=100, max_font_size=50)\n",
    "    wc.generate(words)\n",
    "    plt.title(\"Most common {} words\".format(satisfaction), fontsize=20)\n",
    "    plt.imshow(wc.recolor(colormap='Pastel2', random_state=17), alpha=0.98)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Clean Reviews\n",
    "foodDelivery['reviews_clean_array']= foodDelivery[\"Reviews\"].map(clean) # (!) adds column in foodDelivery - array of cleaned words\n",
    "foodDelivery['reviews_clean']= foodDelivery['reviews_clean_array'].apply(lambda x: \" \".join([str(word) for word in x])) # (!) adds column in foodDelivery - cleaned words in a string\n",
    "\n",
    "positive_words=\" \".join(foodDelivery[foodDelivery.Output == 1]['reviews_clean'].values)\n",
    "negative_words=\" \".join(foodDelivery[foodDelivery.Output == 0]['reviews_clean'].values)\n",
    "\n",
    "generate_wordcloud(positive_words,\"Positive\")\n",
    "generate_wordcloud(negative_words,\"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795b1be",
   "metadata": {},
   "source": [
    "Secondly, using Bag of Words, we perform Sentiment Analysis based on rareness of positive and negative words.  \n",
    "Sklearn provides 3 ways to do this:  \n",
    "> - CountVectorizer simply converts the array of words into an array of 1s and 0s\n",
    "> - TfidfVectorizer (Term-Frequency Inverse-Document-Frequency) builts on CountVectorizer by calculating word frequencies and word importance\n",
    "> - HashingVectorizer builts on TfidfVectorizer by hashing the words instead. This is useful for very large sets of words   \n",
    "\n",
    "We will be using TfidfVectorizer since it is better than CountVectorizer in Sentiment Analysis and our dataset is not that large.  \n",
    "\n",
    "TfidfVectorizer will calculate the Term Frequency(Number of times a word appears in a sentence) and Inverse Document Frequency (How rare or common a word is in a sentence) to derive TF-IDF  \n",
    "Term Frequency tf(t, d) = (Number of times term t appears in a document) / (Total number of terms in the document)  \n",
    "Inverse Document Frequency idf(t) = log ( Number of sentences / df(t) ) + 1   \n",
    "TF-IDF(t, d) = tf(t, d) * idf(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming DataFrame of TDIDF values from all 388 reviews\n",
    "def vectorize(data, tfidf_vect_fit):\n",
    "    X_tfidf = tfidf_vect_fit.transform(data)\n",
    "    words = tfidf_vect_fit.get_feature_names()\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "    X_tfidf_df.columns = words\n",
    "    return(X_tfidf_df)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_features = 1000) # TDIDF Vectorizer\n",
    "tfidf_vect_fit = tfidf_vect.fit(foodDelivery['reviews_clean']) # Fitting string of words into sparse matrix\n",
    "reviews_vectorised = vectorize(foodDelivery['reviews_clean'], tfidf_vect_fit)\n",
    "reviews_vectorised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990ccb2",
   "metadata": {},
   "source": [
    "## Method 2: Pattern\n",
    "---\n",
    "The Pattern Python library is useful in performing NLP.  \n",
    "Its all-in-one data cleaning Parse function can easily clean data with features as inputs, and the Sentiment function converts a sentence into two values, sentiment and subjectivity, for modelling. We will be using both in our analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b01db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating with the rest of the variables\n",
    "# FeatureUnion?\n",
    "foodDelivery = pd.concat([foodDelivery, reviews_vectorised], axis = 1)\n",
    "foodDelivery = foodDelivery.drop(columns = [\"reviews_clean_array\", \"reviews_clean\", \"Reviews\"])\n",
    "foodDelivery.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501188c",
   "metadata": {},
   "source": [
    "# Regression Modelling\n",
    "To answer our question, we have chosen Logistic Regression and Random Forest regression models and have used Cross-Validation GridSearch to improve each of our models. \n",
    "\n",
    "### ROC_AUC\n",
    "For our models, we shall use ROC-AUC score to determine the accuracy of the models.\n",
    "\n",
    "The ROC-AUC is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
    "\n",
    "The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n",
    "\n",
    "When AUC = 1, then the classifier is able to perfectly distinguish between all the Positive and the Negative class points correctly. On the other hand, if the AUC = 0, then the classifier would be predicting all Negatives as Positives, and all Positives as Negatives.\n",
    "\n",
    "This means that the higher the ROC-AUC value, the model is better able to distinguish positives and negatives.\n",
    "\n",
    "### Logistic Regression\n",
    "The reason why we used Logistic Regression rather than using Linear Regression is because \n",
    "Linear Regression is a supervised Machine Learning algorithm that predicts continuous values. \n",
    "\n",
    "On the other hand, Logistic Regression is another supervised Machine Learning algorithm that helps fundamentally in binary classification. \n",
    "\n",
    "In our case, since Output consists of Yes/No, we have decided to use Logistic Regression.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = foodDelivery.drop(['Output'], axis = 1)\n",
    "sc = StandardScaler()\n",
    "Input = sc.fit_transform(Input)\n",
    "Output = foodDelivery['Output']\n",
    "\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(Input, Output, test_size = 0.20, random_state = 0)\n",
    "logreg = LogisticRegression(random_state=0, max_iter = 1000)\n",
    "logreg.fit(X_train_log, y_train_log)\n",
    "\n",
    "y_train_pred_log = logreg.predict(X_train_log)\n",
    "y_test_pred_log = logreg.predict(X_test_log)\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", logreg.score(X_train_log, y_train_log))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", metrics.mean_squared_error(y_train_log, y_train_pred_log))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(metrics.mean_squared_error(y_train_log, y_train_pred_log)))\n",
    "print()\n",
    "\n",
    "\n",
    "TN_train_log = metrics.confusion_matrix(y_train_log, y_train_pred_log)[0][0]\n",
    "FP_train_log = metrics.confusion_matrix(y_train_log, y_train_pred_log)[0][1]\n",
    "FN_train_log = metrics.confusion_matrix(y_train_log, y_train_pred_log)[1][0]\n",
    "TP_train_log = metrics.confusion_matrix(y_train_log, y_train_pred_log)[1][1]\n",
    "\n",
    "FPRate_train_log = FP_train_log / (TN_train_log + FP_train_log)\n",
    "FNRate_train_log = FN_train_log / (TP_train_log + FN_train_log)\n",
    "print(\"False Positive Rate \\t\\t:\", FPRate_train_log)\n",
    "print(\"True Positive Rate \\t\\t:\", 1 - FNRate_train_log)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_train_log, y_train_pred_log))\n",
    "print(\"Precision:\",metrics.precision_score(y_train_log, y_train_pred_log))\n",
    "print(\"Recall:\",metrics.recall_score(y_train_log, y_train_pred_log))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", logreg.score(X_test_log, y_test_log))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", metrics.mean_squared_error(y_test_log, y_test_pred_log))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(metrics.mean_squared_error(y_test_log, y_test_pred_log)))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "TN_test_log = metrics.confusion_matrix(y_test_log, y_test_pred_log)[0][0]\n",
    "FP_test_log = metrics.confusion_matrix(y_test_log, y_test_pred_log)[0][1]\n",
    "FN_test_log = metrics.confusion_matrix(y_test_log, y_test_pred_log)[1][0]\n",
    "TP_test_log = metrics.confusion_matrix(y_test_log, y_test_pred_log)[1][1]\n",
    "\n",
    "FPRate_test_log = FP_test_log / (TN_test_log + FP_test_log)\n",
    "FNRate_test_log = FN_test_log / (TP_test_log + FN_test_log)\n",
    "print(\"False Positive Rate \\t\\t:\", FPRate_test_log)\n",
    "print(\"True Positive Rate \\t\\t:\", 1 - FNRate_test_log)\n",
    "print()\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_log, y_test_pred_log))\n",
    "print(\"Precision:\",metrics.precision_score(y_test_log, y_test_pred_log))\n",
    "print(\"Recall:\",metrics.recall_score(y_test_log, y_test_pred_log))\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(metrics.confusion_matrix(y_train_log, y_train_pred_log),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(metrics.confusion_matrix(y_test_log, y_test_pred_log), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1653fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_log = logreg.predict_proba(X_test_log)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test_log, y_pred_proba_log)\n",
    "auc = metrics.roc_auc_score(y_test_log, y_pred_proba_log)\n",
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "plt.plot(fpr,tpr,label=\"LogReg, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234c543",
   "metadata": {},
   "source": [
    "### From the above plot, the ROC-AUC score is 95.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81eee10",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters\n",
    "After building our Logistic Regression model, we shall attempt to use cross validation method RepeatedStratifiedKFold, which repeats Stratified K-Fold n times with different randomizations in each repetition.\n",
    "\n",
    "Thereafter, GridSearch is used to tune the hyperparameters for Logistic Regression.\n",
    "The hyperparameters are:\n",
    "- solvers\n",
    "- penalty\n",
    "- c_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0dc0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# tuning the hyperparameters for Logistic Regression\n",
    "# hyperparameters: solvers, penalty and c-values\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag','saga']\n",
    "penalty = ['none', 'l1', 'l2', 'elasticnet']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train_log, y_train_log)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabda4e3",
   "metadata": {},
   "source": [
    "---\n",
    "### Results of Cross Validation and Grid Search:\n",
    "From the CrossValidation and GridSearch, the following hyperparameters are found to give the best score:\n",
    "- C = 0.1\n",
    "- penalty = l2\n",
    "- solver = saga\n",
    "\n",
    "We shall then tune the hyperparameters of the Logistic Regression accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7246c01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=0, C=0.1, penalty = 'l2', solver='saga', max_iter = 1000)\n",
    "logreg.fit(X_train_log, y_train_log)\n",
    "\n",
    "y_train_pred_log = logreg.predict(X_train_log)\n",
    "y_test_pred_log = logreg.predict(X_test_log)\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", logreg.score(X_train_log, y_train_log))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", metrics.mean_squared_error(y_train_log, y_train_pred_log))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(metrics.mean_squared_error(y_train_log, y_train_pred_log)))\n",
    "print()\n",
    "\n",
    "\n",
    "TN_train_log = metrics.confusion_matrix(y_train_log, y_train_pred_log)[0][0]\n",
    "FP_train_log = metrics.confusion_matrix(y_train_log, y_train_pred_log)[0][1]\n",
    "FN_train_log = metrics.confusion_matrix(y_train_log, y_train_pred_log)[1][0]\n",
    "TP_train_log = metrics.confusion_matrix(y_train_log, y_train_pred_log)[1][1]\n",
    "\n",
    "FPRate_train_log = FP_train_log / (TN_train_log + FP_train_log)\n",
    "FNRate_train_log = FN_train_log / (TP_train_log + FN_train_log)\n",
    "print(\"False Positive Rate \\t\\t:\", FPRate_train_log)\n",
    "print(\"True Positive Rate \\t\\t:\", 1 - FNRate_train_log)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", logreg.score(X_test_log, y_test_log))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", metrics.mean_squared_error(y_test_log, y_test_pred_log))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(metrics.mean_squared_error(y_test_log, y_test_pred_log)))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "TN_test_log = metrics.confusion_matrix(y_test_log, y_test_pred_log)[0][0]\n",
    "FP_test_log = metrics.confusion_matrix(y_test_log, y_test_pred_log)[0][1]\n",
    "FN_test_log = metrics.confusion_matrix(y_test_log, y_test_pred_log)[1][0]\n",
    "TP_test_log = metrics.confusion_matrix(y_test_log, y_test_pred_log)[1][1]\n",
    "\n",
    "FPRate_test_log = FP_test_log / (TN_test_log + FP_test_log)\n",
    "FNRate_test_log = FN_test_log / (TP_test_log + FN_test_log)\n",
    "print(\"False Positive Rate \\t\\t:\", FPRate_test_log)\n",
    "print(\"True Positive Rate \\t\\t:\", 1 - FNRate_test_log)\n",
    "print()\n",
    "\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(metrics.confusion_matrix(y_train_log, y_train_pred_log),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(metrics.confusion_matrix(y_test_log, y_test_pred_log), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822854b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_log = logreg.predict_proba(X_test_log)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test_log, y_pred_proba_log)\n",
    "auc = metrics.roc_auc_score(y_test_log, y_pred_proba_log)\n",
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "plt.plot(fpr,tpr,label=\"Tuned LogReg, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc52633",
   "metadata": {},
   "source": [
    "### Improvement!\n",
    "From the ROC-AUC score, we can see that after the tuning the hyperparameters using Cross Validation and Grid Search, our model has improved, as the ROC-AUC score has improved from 95.3% to 96.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a4e8d",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "---\n",
    "Similar to Logistic Regression, Random Forest Classifier is another model that makes use of the averages of many decision trees to do binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53adcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = foodDelivery.drop(['Output'], axis = 1)\n",
    "#sc = StandardScaler()\n",
    "#Input = sc.fit_transform(Input)\n",
    "Output = foodDelivery['Output']\n",
    "\n",
    "X_train_forest, X_test_forest, y_train_forest, y_test_forest = train_test_split(Input, Output, test_size = 0.20, random_state = 0)\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X_train_forest, y_train_forest)\n",
    "\n",
    "y_train_pred_forest = forest.predict(X_train_forest)\n",
    "y_test_pred_forest = forest.predict(X_test_forest)\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", forest.score(X_train_forest, y_train_forest))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", metrics.mean_squared_error(y_train_forest, y_train_pred_forest))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(metrics.mean_squared_error(y_train_forest, y_train_pred_forest)))\n",
    "print()\n",
    "\n",
    "\n",
    "TN_train_forest = metrics.confusion_matrix(y_train_forest, y_train_pred_forest)[0][0]\n",
    "FP_train_forest = metrics.confusion_matrix(y_train_forest, y_train_pred_forest)[0][1]\n",
    "FN_train_forest = metrics.confusion_matrix(y_train_forest, y_train_pred_forest)[1][0]\n",
    "TP_train_forest = metrics.confusion_matrix(y_train_forest, y_train_pred_forest)[1][1]\n",
    "\n",
    "FPRate_train_forest = FP_train_forest / (TN_train_forest + FP_train_forest)\n",
    "FNRate_train_forest = FN_train_forest / (TP_train_forest + FN_train_forest)\n",
    "print(\"False Positive Rate \\t\\t:\", FPRate_train_forest)\n",
    "print(\"True Positive Rate \\t\\t:\", 1 - FNRate_train_forest)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_train_forest, y_train_pred_forest))\n",
    "print(\"Precision:\",metrics.precision_score(y_train_forest, y_train_pred_forest))\n",
    "print(\"Recall:\",metrics.recall_score(y_train_forest, y_train_pred_forest))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", logreg.score(X_test_forest, y_test_forest))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", metrics.mean_squared_error(y_test_forest, y_test_pred_forest))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(metrics.mean_squared_error(y_test_forest, y_test_pred_forest)))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "TN_test_forest = metrics.confusion_matrix(y_test_forest, y_test_pred_forest)[0][0]\n",
    "FP_test_forest = metrics.confusion_matrix(y_test_forest, y_test_pred_forest)[0][1]\n",
    "FN_test_forest = metrics.confusion_matrix(y_test_forest, y_test_pred_forest)[1][0]\n",
    "TP_test_forest = metrics.confusion_matrix(y_test_forest, y_test_pred_forest)[1][1]\n",
    "\n",
    "FPRate_test_forest = FP_test_forest / (TN_test_forest + FP_test_forest)\n",
    "FNRate_test_forest = FN_test_forest / (TP_test_forest + FN_test_forest)\n",
    "print(\"False Positive Rate \\t\\t:\", FPRate_test_forest)\n",
    "print(\"True Positive Rate \\t\\t:\", 1 - FNRate_test_forest)\n",
    "print()\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_forest, y_test_pred_forest))\n",
    "print(\"Precision:\",metrics.precision_score(y_test_forest, y_test_pred_forest))\n",
    "print(\"Recall:\",metrics.recall_score(y_test_forest, y_test_pred_forest))\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(metrics.confusion_matrix(y_train_forest, y_train_pred_forest),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(metrics.confusion_matrix(y_test_forest, y_test_pred_forest), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_forest = forest.predict_proba(X_test_forest)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test_forest, y_pred_proba_forest)\n",
    "auc = metrics.roc_auc_score(y_test_forest, y_pred_proba_forest)\n",
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "plt.plot(fpr,tpr,label=\"RandomForestClassifier, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a1fa7",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters\n",
    "Likewise, we shall attempt to improve on the model using Cross Validation and GridSearch to tune the hyperparameters for RandomForest.\n",
    "The hyperparameters are:\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 200, 300, 400]\n",
    "max_depth = [i for i in range(5,15)]\n",
    "max_features= ['sqrt', 'log2']\n",
    "\n",
    "grid = dict(n_estimators=n_estimators,max_depth=max_depth, max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=forest, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train_log, y_train_log)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d4adb",
   "metadata": {},
   "source": [
    "### Results of Cross Validation and Grid Search:\n",
    "From the CrossValidation and GridSearch, the following hyperparameters are found to give the best score for RandomForestClassifier:\n",
    "- max_features = sqrt\n",
    "- n_estimators = 400\n",
    "- max_depth = 13\n",
    "\n",
    "We shall then tune the hyperparameters of the RandomForest accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_features='sqrt', n_estimators=400, max_depth = 13,random_state=0)\n",
    "forest.fit(X_train_forest, y_train_forest)\n",
    "\n",
    "y_train_pred_forest = forest.predict(X_train_forest)\n",
    "y_test_pred_forest = forest.predict(X_test_forest)\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", forest.score(X_train_forest, y_train_forest))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", metrics.mean_squared_error(y_train_forest, y_train_pred_forest))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(metrics.mean_squared_error(y_train_forest, y_train_pred_forest)))\n",
    "print()\n",
    "\n",
    "\n",
    "TN_train_forest = metrics.confusion_matrix(y_train_forest, y_train_pred_forest)[0][0]\n",
    "FP_train_forest = metrics.confusion_matrix(y_train_forest, y_train_pred_forest)[0][1]\n",
    "FN_train_forest = metrics.confusion_matrix(y_train_forest, y_train_pred_forest)[1][0]\n",
    "TP_train_forest = metrics.confusion_matrix(y_train_forest, y_train_pred_forest)[1][1]\n",
    "\n",
    "FPRate_train_forest = FP_train_forest / (TN_train_forest + FP_train_forest)\n",
    "FNRate_train_forest = FN_train_forest / (TP_train_forest + FN_train_forest)\n",
    "print(\"False Positive Rate \\t\\t:\", FPRate_train_forest)\n",
    "print(\"True Positive Rate \\t\\t:\", 1 - FNRate_train_forest)\n",
    "\n",
    "print()\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", logreg.score(X_test_forest, y_test_forest))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", metrics.mean_squared_error(y_test_forest, y_test_pred_forest))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(metrics.mean_squared_error(y_test_forest, y_test_pred_forest)))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "TN_test_forest = metrics.confusion_matrix(y_test_forest, y_test_pred_forest)[0][0]\n",
    "FP_test_forest = metrics.confusion_matrix(y_test_forest, y_test_pred_forest)[0][1]\n",
    "FN_test_forest = metrics.confusion_matrix(y_test_forest, y_test_pred_forest)[1][0]\n",
    "TP_test_forest = metrics.confusion_matrix(y_test_forest, y_test_pred_forest)[1][1]\n",
    "\n",
    "FPRate_test_forest = FP_test_forest / (TN_test_forest + FP_test_forest)\n",
    "FNRate_test_forest = FN_test_forest / (TP_test_forest + FN_test_forest)\n",
    "print(\"False Positive Rate \\t\\t:\", FPRate_test_forest)\n",
    "print(\"True Positive Rate \\t\\t:\", 1 - FNRate_test_forest)\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(metrics.confusion_matrix(y_train_forest, y_train_pred_forest),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(metrics.confusion_matrix(y_test_forest, y_test_pred_forest), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_forest = forest.predict_proba(X_test_forest)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test_forest, y_pred_proba_forest)\n",
    "auc = metrics.roc_auc_score(y_test_forest, y_pred_proba_forest)\n",
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "plt.plot(fpr,tpr,label=\"Tuned RandomForestClassifier, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfd5aa",
   "metadata": {},
   "source": [
    "### Improvement again!\n",
    "Similar to to our previous Cross Validation and GridSearch of the Logistic Regression model, we can see that after the tuning the hyperparameters using Cross Validation and Grid Search, our model has improved, as the ROC-AUC score has improved from 97.5% to 97.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d1eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
